---
title: Telemetry Aware Scheduler
abbrlink: 195363fe
date: 2020-08-28 16:53:25
categories: [Software, Kubernetes]
tags:
- Kubernetes
---
* TAS 包含兩個 container
  * tasext
a TAS Scheduler Extender which is an extension to K8s scheduler
  * tascont
a TAS Policy Controller which consumes TAS policies
* Create a secret that allows contact with TAS
In a production cluster, these certs should be set up as per normal cluster security policy.
```
kubectl create secret tls extender-secret --cert /etc/kubernetes/pki/ca.crt --key /etc/kubernetes/pki/ca.key
```
Error message in tasext
```
{"log":"2020/08/28 00:45:41 http: TLS handshake error from 10.0.2.15:35194: remote error: tls: unknown certificate authority\n","stream":"stderr","time":"2020-08-28T00:45:41.423020853Z"}
```
* Update the configuration of the core K8s scheduler to configure a cluster for TAS deployment
主要是在 `scheduler-extender-configmap.yaml` 更新 `tas-service ip`
  * tas-service information
```
NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                       AGE
tas-service                ClusterIP   10.104.110.87    <none>        9001/TCP                      23h
```
  * configure-scheduler.sh
```
MANIFEST_FILE=/etc/kubernetes/manifests/kube-scheduler.yaml
CONFIG_MAP=scheduler-extender-configmap.yaml
CLUSTER_ROLE=configmap-getter.yaml

kubectl apply -f $CONFIG_MAP
kubectl apply -f $CLUSTER_ROLE
kubectl create clusterrolebinding scheduler-config-map --clusterrole=configmapgetter --user=system:kube-scheduler

sed -i '/^    - --policy-configmap/d' $MANIFEST_FILE
sed -i '/^  dnsPolicy: ClusterFirstWithHostNet/d' $MANIFEST_FILE
sed -e "/    - kube-scheduler/a\\
    - --policy-configmap=scheduler-extender-policy\n    - --policy-configmap-namespace=kube-system" $MANIFEST_FILE -i
sed -e "/spec/a\\
  dnsPolicy: ClusterFirstWithHostNet" $MANIFEST_FILE -i
```
  * configmap-getter.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: configmapgetter
  namespace: kube-system 
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
```
  * scheduler-extender-configmap.yaml
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-extender-policy
  namespace: kube-system
data:
  policy.cfg: |
    {
        "kind" : "Policy",
        "apiVersion" : "v1",
        "extenders" : [
            {
              "urlPrefix": "https://10.104.110.87:9001",
              "apiVersion": "v1",
              "prioritizeVerb": "scheduler/prioritize",
              "filterVerb": "scheduler/filter",
              "weight": 1,
              "enableHttps": true,
              "managedResources": [
                   {
                     "name": "telemetry/scheduling",
                     "ignoredByScheduler": true
                   }
              ],
              "ignorable": true
          }
         ]
    }
```
  * apply
```
$ ./configure-scheduler.sh
configmap/scheduler-extender-policy configured
clusterrole.rbac.authorization.k8s.io/configmapgetter unchanged
Error from server (AlreadyExists): clusterrolebindings.rbac.authorization.k8s.io "scheduler-config-map" already exists
```
* Create TASPolicy
  * get available metric
```
$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .
```
  * get value of target metric
```
$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/nodes/*/memory_MemFree_bytes | jq .
```
  * policy.yaml
```bash
apiVersion: telemetry.intel.com/v1alpha1
kind: TASPolicy
metadata:
  name: demo-policy
  namespace: default
spec:
  strategies:
    deschedule:
      rules:
      - metricname: memory_MemFree_bytes
        operator: LessThan
        target: 10000000
    dontschedule:
      rules:
      - metricname: memory_MemFree_bytes
        operator: LessThan
        target: 1000000
    scheduleonmetric:
      rules:
      - metricname: memory_MemFree_bytes
        operator: GreaterThan
```
  * log of adding a policy in tascont
會一直去 monitor memory_MemFree_bytes 的值
```
{"log":"2020/08/28 09:02:50 registering scheduleonmetric from demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.856944522Z"}
{"log":"2020/08/28 09:02:50 Adding strategies: scheduleonmetric demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.856986479Z"}
{"log":"2020/08/28 09:02:50 Added memory_MemFree_bytes\n","stream":"stderr","time":"2020-08-28T09:02:50.856995355Z"}
{"log":"2020/08/28 09:02:50 registering dontschedule from demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.857002389Z"}
{"log":"2020/08/28 09:02:50 Adding strategies: dontschedule demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.857008778Z"}
{"log":"2020/08/28 09:02:50 Added memory_MemFree_bytes\n","stream":"stderr","time":"2020-08-28T09:02:50.857014218Z"}
{"log":"2020/08/28 09:02:50 registering deschedule from demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.857019374Z"}
{"log":"2020/08/28 09:02:50 Adding strategies: deschedule demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.857024892Z"}
{"log":"2020/08/28 09:02:50 Added memory_MemFree_bytes\n","stream":"stderr","time":"2020-08-28T09:02:50.85703006Z"}
{"log":"2020/08/28 09:02:50 Added policy, demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:50.857034987Z"}
{"log":"2020/08/28 09:02:52 Evaluating demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:52.302196222Z"}
{"log":"2020/08/28 09:02:52 no metric memory_MemFree_bytes found\n","stream":"stderr","time":"2020-08-28T09:02:52.302220044Z"}
{"log":"2020/08/28 09:02:54 Evaluating demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:54.38687469Z"}
{"log":"2020/08/28 09:02:54 openness memory_MemFree_bytes = 194686976\n","stream":"stderr","time":"2020-08-28T09:02:54.3869245Z"}
{"log":"2020/08/28 09:02:56 Evaluating demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:56.303365136Z"}
{"log":"2020/08/28 09:02:56 openness memory_MemFree_bytes = 194686976\n","stream":"stderr","time":"2020-08-28T09:02:56.303406196Z"}
```
* log of deleting a policy in tascont
```
{"log":"2020/08/28 09:02:56 Removed demo-policy: deschedule from strategy register\n","stream":"stderr","time":"2020-08-28T09:02:56.865109714Z"}
{"log":"2020/08/28 09:02:56 Removed demo-policy: scheduleonmetric from strategy register\n","stream":"stderr","time":"2020-08-28T09:02:56.865133184Z"}
{"log":"2020/08/28 09:02:56 Removed demo-policy: dontschedule from strategy register\n","stream":"stderr","time":"2020-08-28T09:02:56.865155074Z"}
{"log":"2020/08/28 09:02:56 deletingpolicies/default/demo-policy\n","stream":"stderr","time":"2020-08-28T09:02:56.865158043Z"}
{"log":"2020/08/28 09:02:56 Policy:  demo-policy  deleted\n","stream":"stderr","time":"2020-08-28T09:02:56.865160837Z"}
```
* Linking a workload to a policy
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-app
  labels:
    app: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
        telemetry-policy: demo-policy
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            telemetry/scheduling: 1
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: demo-policy
                    operator: NotIn
                    values:
                      - violating
```
  * log of adding a pod with demo-policy in tasext
    * 允許部署
```
{"log":"2020/08/28 09:05:04 filter request recieved\n","stream":"stderr","time":"2020-08-28T09:05:04.323434954Z"}
{"log":"2020/08/28 09:05:04 openness memory_MemFree_bytes = 160436224\n","stream":"stderr","time":"2020-08-28T09:05:04.332601831Z"}
{"log":"2020/08/28 09:05:04 Filtered nodes for demo-policy : openness \n","stream":"stderr","time":"2020-08-28T09:05:04.332642768Z"}
```
    * 不允許部署
```
{"log":"2020/08/28 09:07:11 filter request recieved\n","stream":"stderr","time":"2020-08-28T09:07:11.429996533Z"}
{"log":"2020/08/28 09:07:11 openness memory_MemFree_bytes = 147173376\n","stream":"stderr","time":"2020-08-28T09:07:11.433672095Z"}
{"log":"2020/08/28 09:07:11 openness violating : memory_MemFree_bytes LessThan 5000000000\n","stream":"stderr","time":"2020-08-28T09:07:11.433697985Z"}
```
  * 不允許部署時，Events of the demo-app pod
```
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/1 nodes are available: 1 Node violates.
```
* Deploy and Configure Kubernetes Descheduler
  * Install descheduler
module requires Go 1.15
```bash
$ git clone https://github.com/kubernetes-sigs/descheduler
$ cd descheduler
$ make
$ cp ./_output/bin/descheduler /usr/bin
```
  * Run the descheduler in order to stop the workloads on the offending nodes
    * descheduler-policy.yaml
```bash
$ cat descheduler-policy.yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemovePodsViolatingNodeAffinity":
    enabled: true
    params:
      nodeAffinityType:
        - "requiredDuringSchedulingIgnoredDuringExecution"
```
    * Apply
```
$ descheduler --logtostderr -v4 --kubeconfig=$HOME/.kube/config --policy-config-file=descheduler-policy.yaml
```
    * Error
```
I0828 10:47:32.515718   29836 node.go:45] node lister returned empty list, now fetch directly
I0828 10:47:32.531210   29836 descheduler.go:106] The cluster size is 0 or 1 meaning eviction causes service disruption or degradation. So aborting..
```
* Reference
https://github.com/intel/telemetry-aware-scheduling